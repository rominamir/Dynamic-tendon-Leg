==========================================
SLURM_JOB_ID = 863316
SLURM_JOB_NODELIST = b01-06
TMPDIR = /tmp/SLURM_863316
==========================================
Run 'mamba init' to be able to run mamba activate/deactivate
and start a new shell session. Or use conda to activate/deactivate.

2025-06-14 12:17:30.325990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home1/jiajinzh/.conda/envs/lab/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.
  warnings.warn(
/home1/jiajinzh/.conda/envs/lab/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
ðŸš€ Training | Seed=100 | Growth=constant:30k | LR=constant
Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Logging to ./tensorboard_log/LegEnv_Jun14_constant_30k_constant_5e-04_PPO_seeds_100-101/ppo/PPO_9
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 271      |
| time/              |          |
|    fps             | 769      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 335         |
| time/                   |             |
|    fps                  | 674         |
|    iterations           | 2           |
|    time_elapsed         | 6           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.005818124 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | -0.00927    |
|    learning_rate        | 0.0005      |
|    loss                 | 14.7        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00427    |
|    std                  | 0.994       |
|    value_loss           | 36.2        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 344          |
| time/                   |              |
|    fps                  | 714          |
|    iterations           | 3            |
|    time_elapsed         | 8            |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0053794365 |
|    clip_fraction        | 0.0437       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.302        |
|    learning_rate        | 0.0005       |
|    loss                 | 19.3         |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00708     |
|    std                  | 0.984        |
|    value_loss           | 59.7         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 318          |
| time/                   |              |
|    fps                  | 736          |
|    iterations           | 4            |
|    time_elapsed         | 11           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0074050906 |
|    clip_fraction        | 0.0848       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.415        |
|    learning_rate        | 0.0005       |
|    loss                 | 25.7         |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.0136      |
|    std                  | 0.991        |
|    value_loss           | 68.2         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 324          |
| time/                   |              |
|    fps                  | 750          |
|    iterations           | 5            |
|    time_elapsed         | 13           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.0044563846 |
|    clip_fraction        | 0.0224       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.359        |
|    learning_rate        | 0.0005       |
|    loss                 | 25.7         |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00516     |
|    std                  | 0.978        |
|    value_loss           | 65           |
------------------------------------------
/home1/jiajinzh/.conda/envs/lab/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.
  warnings.warn(
Model saved at: ./data/LegEnv_Jun14_constant_30k_constant_5e-04_PPO_seeds_100-101/final_model_seed_100.zip
Saved reward: [214.80709876183448, 327.31211343181235, 304.60059740195425, 494.526344304471, 343.5384109822111]... length=10
Saved reward and displacement to ./data/LegEnv_Jun14_constant_30k_constant_5e-04_PPO_seeds_100-101/distance/
ðŸ”§ Creating LegEnvBase...
âœ… Created LegEnvBase: <class 'env.LegEnvBase'>
Eval env type: <class 'env.LegEnvBase'>
âœ… Finished training for Seed 100
ðŸš€ Training | Seed=101 | Growth=constant:30k | LR=constant
Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Logging to ./tensorboard_log/LegEnv_Jun14_constant_30k_constant_5e-04_PPO_seeds_100-101/ppo/PPO_10
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 393      |
| time/              |          |
|    fps             | 1170     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 347         |
| time/                   |             |
|    fps                  | 954         |
|    iterations           | 2           |
|    time_elapsed         | 4           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.010703948 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.00564     |
|    learning_rate        | 0.0005      |
|    loss                 | 29.9        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0129     |
|    std                  | 0.986       |
|    value_loss           | 80.6        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 339         |
| time/                   |             |
|    fps                  | 902         |
|    iterations           | 3           |
|    time_elapsed         | 6           |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.008064593 |
|    clip_fraction        | 0.0817      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.315       |
|    learning_rate        | 0.0005      |
|    loss                 | 41.5        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00853    |
|    std                  | 0.973       |
|    value_loss           | 77.1        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 331         |
| time/                   |             |
|    fps                  | 879         |
|    iterations           | 4           |
|    time_elapsed         | 9           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.005161946 |
|    clip_fraction        | 0.069       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.402       |
|    learning_rate        | 0.0005      |
|    loss                 | 35.9        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00841    |
|    std                  | 0.97        |
|    value_loss           | 69.7        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 343         |
| time/                   |             |
|    fps                  | 866         |
|    iterations           | 5           |
|    time_elapsed         | 11          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.010217143 |
|    clip_fraction        | 0.0721      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.424       |
|    learning_rate        | 0.0005      |
|    loss                 | 18.9        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00887    |
|    std                  | 0.958       |
|    value_loss           | 58.2        |
-----------------------------------------
Model saved at: ./data/LegEnv_Jun14_constant_30k_constant_5e-04_PPO_seeds_100-101/final_model_seed_101.zip
Saved reward: [388.27373020824393, 398.11756255222576, 170.07598814432475, 432.56287017003956, 276.71069794115044]... length=10
Saved reward and displacement to ./data/LegEnv_Jun14_constant_30k_constant_5e-04_PPO_seeds_100-101/distance/
ðŸ”§ Creating LegEnvBase...
âœ… Created LegEnvBase: <class 'env.LegEnvBase'>
Eval env type: <class 'env.LegEnvBase'>
âœ… Finished training for Seed 101
ðŸ“Š Aggregating results from all seeds...
Saved aggregated reward mean and SE at ./data/aggregated_results/LegEnv_Jun14_constant_30k_constant_5e-04_PPO_seeds_100-101/ with growth type 'constant_30k'
Saved aggregated displacement mean and SE at ./data/aggregated_results/LegEnv_Jun14_constant_30k_constant_5e-04_PPO_seeds_100-101/ with growth type 'constant_30k'
âœ… Aggregation complete.
